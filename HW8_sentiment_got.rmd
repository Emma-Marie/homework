---
title: "HW8_sentiment_got.rmd"
author: "Emma-Marie"
date: 'Created 8 November 2022 and updated `r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

### Loading packages 
Downloading the needed packages and run line 29 and 31


```{r packages}

library(tidyverse)
library(here)
library(dplyr)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

# Note - Before lab:
# Attach tidytext and textdata packages
# Run: get_sentiments(lexicon = "nrc") 
# Should be prompted to install lexicon - choose yes!
# Run: get_sentiments(lexicon = "afinn") 
# Should be prompted to install lexicon - choose yes!
```

### Get the Game of Thrones pdf:

```{r get-document}
got_path <- here("data","got.pdf")
got_text <- pdf_text(got_path)
```

Example: Just want to get text from a single page (e.g. Page 8)

```{r single-page}
got_p8 <- got_text[8]

```
### Some wrangling:
 Splitting the pages into seperate lines so that each line on each page has its own row. Extra starting & trailing spaces are also removed.

```{r spliting lines}
got_df <- data.frame(got_text) %>% #making the got_text into a data frame.
  mutate(text_full = str_split(got_text, pattern = '\\n')) %>% #split text into separate lines
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) #str_trim() trim beginning spaces and trailing spaces

```
### Get the tokens (individual words) in tidy format

Now I split the columns into tokens using the `tidytext::unnest_tokens()`. I am interested in the words. 

```{r tokenize}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full) # break full text into individual words
# now there is one word per line and each word has its own row!

```
Now I can count the words in the pdf

```{r count words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)

#View the words
got_wc

```
The most frequent words in the pdf is not that interesting e.g. "the", "and", "to" etc, and I would therefore like to remove them using a stop word list. 

### Remove stop words:

I will remove the stop words using `tidyr::anti_join()`

```{r removing stop words}
got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% # removes the stop words
  select(-got_text)

```
I now count the words again (but this time without the stop words).

```{r count words 2}

got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)

#View the words
got_swc

```
"Lord" and "ser" are the two most frequent occurring words in the text.

### A word cloud of got report words

I would like to make a word cloud of the 100 most used words from the got text.

```{r wordcloud-prep}

length(unique(got_stop$word)) #there are 11295 unique words in the got text

# Filtering the list of words from the text to only include the top 100 most frequent in my word cloud.
got_top100 <- got_stop %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)

```


```{r wordcloud}
got_cloud <- ggplot(data = got_top100, aes(label = word)) + #making the word cloud
  geom_text_wordcloud() +
  theme_minimal()

```

This word cloud isn't that informative, so I will customize it with colours, shape and sizes to give a better view of the most used words:

```{r wordcloud-pro}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

Now its clear to see that "lord", "ser", "jon", "ned", "hand", "eyes", "told", "bran" and "father" are among the most frequent words (and roughly in that order). 

### Sentiment analysis

I will download the lexicon "afinn" to be able to rank the words from -5 (very negative) to +5 (very positive).

```{r afinn}
get_sentiments(lexicon = "afinn") #get_sentiments() gets the lexicon without the columns not used in the particular lexicon

# Getting the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Getting the pretty negative words: 
afinn_neg <- get_sentiments("afinn") %>% 
  filter(value %in% c(-5,-4,-3))
```
I will download the lexicon "bing" to be able to sort the words in "positive" or "negative" words (binary sorting the words):

```{r bing}
get_sentiments(lexicon = "bing")
```
I get the NRC lexicon to be able to sort the words into bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative. 

**Citation for NRC lexicon**: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

```{r nrc}
get_sentiments(lexicon = "nrc")
```
### Sentiment analysis with afinn: 

I bind the words in `got_stop` (the list of words without the stop words) to the `afinn` lexicon:

```{r afinn bind}

got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))

```

I count the words in each category (from -5 (most negative) to 5 (most positive)) and plot them. I haven't actually read or watched Game of Thrones, but I have heard, that a lot of warfare and conflict is going on, so therefore I expect that the words will generally be slightly more negative than positive. 

```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col()

```

The most words are placed in the -2 category, which contains slightly negative words. 

Now I will investigate the words in category -2 a bit more:

```{r afinn -2}
# What are the words in category '-2'?
got_afinn_minus2 <- got_afinn %>% 
  filter(value == -2)

```


```{r affin 2 more}
# Check the unique -2-score words:
unique(got_afinn_minus2$word)

# Count & plot them
got_afinn_minus2_n <- got_afinn_minus2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn_minus2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

```

The plot is a bit hard to read, because there are so many words in the -2 category. Therefore I will check out how many times the individual -2 words occur and which words are most frequent. 

```{r word count -2 words}
got_minus2_swc <- got_afinn_minus2 %>% 
  count(word) %>% 
  arrange(-n)

#checking the words out
got_minus2_swc

```
"Fire", "fear", "death", "afraid", "pain", "tears", "hurt", "fool", "wrong" and "war" are the 10 most frequent words in category -2. It is worth noticing that fire isn't necessary a negative word, because it can be quite positive to light up a fire in your fireplace or camp to keep warm or cook dinner. "Tears" can also be positive e.g. if you cry of happiness. 


I will summarize sentiment for the report: 
```{r summarize-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

Result: the mean (-0.542) and median (-1) scores indicate that the words are generally slightly negative in the Game of Thrones text.

### NRC lexicon for sentiment analysis

We can use the NRC lexicon to start "binning" the got text by the feelings they're typically associated with. I will use inner_join() to combine the got text without the stop words with the nrc lexicon: 

```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```

Before counting the words I will check which words has now been excluded from the text

```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(got_exclude)

# Count to find the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```

The most excluded words are "ser", some names ("Ned", "Jon" and "Tyron") and the words "eyes" and "hand" which are two quite neutral words I would say. 

Now I count the words in the bins and plot them
```{r count-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()
```

The analysis shows that the bin with the most words are the "positive" bin. But the "negative" bin is almost as big, and a lot of other bins with negative connotations such as "anger", "disgust" and "fear" are also containing a lot of words.

I will count the words by sentiment *and* word, and then facet them:
```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
got_nrc_gg

# Save it
ggsave(plot = got_nrc_gg, 
       here("figures","got_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```
The word "lord" is ambugous, because it occures four of the bins, so the word is both binned as "disgust", "negative", "positive" and "trust".


Checking if Lord is really in four bins
```{r nrc-confidence}
lord <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "lord")

# Check it out
lord # yep, lord is both in the disgust, negative, positive and trust bin
```
Some things that surprises me from the chard:

Why is "stone" negative and not neutral?
Why is "words" anger? It depends on the particular words.
Why is "found" binned as joy? It must depend on what you find. 
Why is "blue" sad? Sometimes its just a colour without any symbolic meaning.



